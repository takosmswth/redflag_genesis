{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Intro and Resource\n",
    "\n",
    "This notebook runs the tensorflow code that trains a GRU model that creates Shakespear with given prompt.\n",
    "\n",
    "Follows the tutorial listed on [this page](https://towardsdatascience.com/create-your-own-artificial-shakespeare-in-10-minutes-with-natural-language-processing-1fde5edc8f28)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing TF and other packages\n",
    "\n",
    "```\n",
    "* TODO: create and activate new Conda Env with python 3.8.5 with following commands\n",
    "    * `conda create -y -n style_gen anaconda python=3.8.5`\n",
    "    * `conda activate style_gen`\n",
    "* (didn't work w Python 3.9.12 LMAO) to install TensorFlow, follow step 3 and 4 in [this SO answer](https://stackoverflow.com/a/68214296/6716783).\n",
    "```\n",
    "\n",
    "* SCRATCH THIS^^^ TRY [this tutorial](https://www.mrdbourke.com/setup-apple-m1-pro-and-m1-max-for-machine-learning-and-data-science/) in your terminal\n",
    "* Make env available for jupyter notebook with:\n",
    "    * `conda install -y -c anaconda ipykernel`\n",
    "        * `python -m ipykernel install --user --name=style_gen_env_tf292`\n",
    "* In DataSpell, choose style_gen_env as the environment to use.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run imports\n",
    "\n",
    "If TF installs correctly, the following cell should run with no error."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tensorflow as tf    # takes a minute or two but will import eventually!\n",
    "import numpy as np\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has access to the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow version: 2.9.2\n"
     ]
    }
   ],
   "source": [
    "# Check for TensorFlow GPU access\n",
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")\n",
    "\n",
    "# See TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt',\n",
    "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the corpus is: 1115394\n",
      "The first 100 characters of the corpus are as follows:\n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read()\n",
    "text = text.decode(encoding='utf-8')\n",
    "print ('Total number of characters in the corpus is:', len(text))\n",
    "print('The first 100 characters of the corpus are as follows:\\n', text[:100])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'/Users/qhong/.keras/datasets/shakespeare.txt'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text Vectorization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique characters in the corpus is 65\n",
      "A slice of the unique characters set:\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3']\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the corpus\n",
    "vocab = sorted(set(text))\n",
    "print ('The number of unique characters in the corpus is', len(vocab))\n",
    "print('A slice of the unique characters set:\\n', vocab[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Create a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "# Make a copy of the unique set elements in NumPy array format for later use in the decoding the predictions\n",
    "idx2char = np.array(vocab)\n",
    "# Vectorize the text with a for loop\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-27 18:31:06.366467: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-27 18:31:06.366802: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "# for i in char_dataset.take(5):\n",
    "#   print(i.numpy())\n",
    "seq_length = 100 # The max. length for single input\n",
    "# examples_per_epoch = len(text)//(seq_length+1) # double-slash for “floor” division\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "# for item in sequences.take(5):\n",
    "#   print(repr(''.join(idx2char[item.numpy()])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 10000 # TF shuffles the data only within buffers\n",
    "\n",
    "BATCH_SIZE = 64 # Batch size\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " gru (GRU)                   (64, None, 1024)          3938304   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size = len(vocab), # no. of unique characters\n",
    "    embedding_dim=embedding_dim, # 256\n",
    "    rnn_units=rnn_units, # 1024\n",
    "    batch_size=BATCH_SIZE)  # 64 for the traning\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compile and Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# # UGH TF is EXTREMELY SLOW without optimization...\n",
    "#\n",
    "# def loss(labels, logits):\n",
    "#     return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "#\n",
    "# # example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "# # print(\"Prediction shape: \", example_batch_predictions.shape, \" (batch_size, sequence_length, vocab_size)\")\n",
    "# # print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
    "#\n",
    "# model.compile(optimizer='adam', loss=loss)\n",
    "#\n",
    "# # Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# # Name of the checkpoint files\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "#\n",
    "# checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_prefix,\n",
    "#     save_weights_only=True)\n",
    "#\n",
    "# EPOCHS = 30\n",
    "# history = model.fit(dataset,\n",
    "#                     epochs=EPOCHS,\n",
    "#                     callbacks=[checkpoint_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "#\n",
    "# def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "#                                   batch_input_shape=[batch_size, None]),\n",
    "#         tf.keras.layers.GRU(rnn_units,\n",
    "#                             return_sequences=True,\n",
    "#                             stateful=True,\n",
    "#                             recurrent_initializer='glorot_uniform'),\n",
    "#         tf.keras.layers.Dense(vocab_size)\n",
    "#     ])\n",
    "#     return model\n",
    "#\n",
    "# # Length of the vocabulary in chars\n",
    "# vocab_size = 65\n",
    "# # The embedding dimension\n",
    "# embedding_dim = 256\n",
    "# # Number of RNN units\n",
    "# rnn_units = 1024\n",
    "#\n",
    "# checkpoint_dir = './training_checkpoints'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 256)            16640     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 65)             66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the work of art in the age of its technological reproducibility gentle\n",
      "We ween Engloness for the manner,\n",
      "And he pais not say we have day repose?\n",
      "\n",
      "GRUMIO:\n",
      "Nay, sir,, as Senot Eill he be man, he did's man's pawith,\n",
      "I would deport you my seem to die your keep' n murder me.\n",
      "Cruple master, if yet old prayers\n",
      "The queen thy so lockst I hid heavy are wated with Jod,\n",
      "If be incle than one comfort thangs,\n",
      "Thus dares lingtweston medes my tongus, that ana\n",
      "Dist for neet image hoposity heast sood.\n",
      "\n",
      "RICHARD:\n",
      "I'll know while. Let thou devise\n",
      "then certoone,\n",
      "And wateration an\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, num_generate, temperature, start_string):\n",
    "    input_eval = [char2idx[s] for s in start_string] # string to numbers (vectorizing)\n",
    "    input_eval = tf.expand_dims(input_eval, 0) # dimension expansion\n",
    "    text_generated = [] # Empty string to store our results\n",
    "    model.reset_states() # Clears the hidden states in the RNN\n",
    "\n",
    "    for i in range(num_generate): #Run a loop for number of characters to generate\n",
    "        predictions = model(input_eval) # prediction for single character\n",
    "        predictions = tf.squeeze(predictions, 0) # remove the batch dimension\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        # higher temperature increases the probability of selecting a less likely character\n",
    "        # lower --> more predictable\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # The predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        # So the model makes the next prediction based on the previous character\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        # Also devectorize the number and add to the generated text\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "generated_text = generate_text(\n",
    "    model,\n",
    "    num_generate=500,\n",
    "    temperature=1,\n",
    "    start_string=u\"the work of art in the age of its technological reproducibility\")\n",
    "print(generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "style_gen_env_tf292",
   "language": "python",
   "display_name": "style_gen_env_tf292"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}